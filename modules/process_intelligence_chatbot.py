import streamlit as st
import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from groq import Groq
import traceback

class ProcessIntelligenceChatbot:
    """Step-aware AI Assistant with proper chat interface and cross-step memory."""
    
    def __init__(self, current_step=None):
        # Current processing step awareness
        self.current_step = current_step or self._detect_current_step()
        
        # Initialize Groq client
        self.client = None
        self._init_groq_client()
        
        # Initialize step-aware chat history
        if 'step_chat_messages' not in st.session_state:
            st.session_state.step_chat_messages = {}
        
        # Ensure current step has message history
        if self.current_step not in st.session_state.step_chat_messages:
            st.session_state.step_chat_messages[self.current_step] = []
        
        # Initialize cross-step memory
        if 'cross_step_memory' not in st.session_state:
            st.session_state.cross_step_memory = {}
        
        # Language settings
        if 'chatbot_language' not in st.session_state:
            st.session_state.chatbot_language = 'english'
        
        # Track first query per step
        if 'step_first_query' not in st.session_state:
            st.session_state.step_first_query = {}
            
        # Context cache with step awareness
        self._context_cache = {}
        self._step_summaries = {}

    def _detect_current_step(self):
        """Detect current processing step based on session state."""
        if st.session_state.get('weighted_results'):
            return 'analysis'
        elif st.session_state.get('cleaned_data') is not None:
            return 'weighting'
        elif st.session_state.get('schema'):
            return 'cleaning'
        elif st.session_state.get('data') is not None:
            return 'schema'
        else:
            return 'upload'
    
    def _init_groq_client(self):
        """Initialize Groq client with silent error handling."""
        try:
            api_key = os.getenv('GROQ_API_KEY')
            if api_key:
                self.client = Groq(api_key=api_key)
            else:
                self.client = None  # Will use fallback responses
        except Exception:
            self.client = None  # Silent fallback

    def _get_step_context(self, step=None):
        """Get context specific to a processing step."""
        target_step = step or self.current_step
        
        if target_step in self._context_cache:
            return self._context_cache[target_step]
            
        context = {
            'step': target_step,
            'data_info': {},
            'step_status': 'pending',
            'previous_steps': [],
            'next_steps': [],
            'recommendations': []
        }
        
        try:
            # Data information with safe handling
            if st.session_state.get('data') is not None:
                data = st.session_state.data
                if data is not None and hasattr(data, 'shape'):
                    context['data_info'] = {
                        'total_rows': int(data.shape[0]),
                        'total_columns': int(data.shape[1]),
                        'missing_values': int(data.isnull().sum().sum()),
                        'missing_percentage': float((data.isnull().sum().sum() / (data.shape[0] * data.shape[1])) * 100),
                        'numeric_columns': int(len(data.select_dtypes(include=[np.number]).columns)),
                        'categorical_columns': int(len(data.select_dtypes(include=['object']).columns)),
                        'duplicates': int(data.duplicated().sum()) if hasattr(data, 'duplicated') else 0
                    }
                    context['step_status'] = 'completed' if target_step == 'upload' else 'available'
            
            # Cleaned data information with safe handling
            if st.session_state.get('cleaned_data') is not None:
                cleaned = st.session_state.cleaned_data
                if cleaned is not None and hasattr(cleaned, 'shape'):
                    original_rows = context['data_info'].get('total_rows', 0)
                    rows_removed = original_rows - cleaned.shape[0] if original_rows > 0 else 0
                    
                    context['cleaning_results'] = {
                        'rows_after_cleaning': int(cleaned.shape[0]),
                        'rows_removed': int(rows_removed),
                        'removal_percentage': float((rows_removed / original_rows * 100)) if original_rows > 0 else 0,
                        'remaining_missing': int(cleaned.isnull().sum().sum()),
                        'quality_improvement': 'significant' if rows_removed > 0 else 'minimal'
                    }
                    if target_step == 'cleaning':
                        context['step_status'] = 'completed'

            # Weighted results
            if st.session_state.get('weighted_results') is not None:
                results = st.session_state.weighted_results
                context['weighting_info'] = {
                    'weights_applied': True,
                    'analysis_completed': True,
                    'statistical_results': 'available'
                }
                if target_step in ['weighting', 'analysis']:
                    context['step_status'] = 'completed'
            
            # Processing log with step awareness
            if st.session_state.get('processing_log'):
                context['previous_steps'] = [
                    entry.get('step', 'Unknown')
                    for entry in st.session_state.processing_log
                ]
            
            # Build step summary for cross-step memory
            self._build_step_summary(target_step, context)
                
        except Exception as e:
            # Graceful fallback
            context['error'] = f"Context extraction error: {str(e)}"
            
        self._context_cache[target_step] = context
        return context
    
    def _build_step_summary(self, step, context):
        """Build compact summary for cross-step memory."""
        summary = {'step': step, 'completed': context.get('step_status') == 'completed'}
        
        if step == 'upload' and context.get('data_info'):
            summary['summary'] = f"Uploaded {context['data_info']['total_rows']} rows, {context['data_info']['total_columns']} columns"
        elif step == 'cleaning' and context.get('cleaning_results'):
            summary['summary'] = f"Cleaned data: {context['cleaning_results']['rows_removed']} rows removed"
        elif step == 'weighting' and context.get('weighting_info'):
            summary['summary'] = "Applied survey weights and calculated statistics"
        else:
            summary['summary'] = f"Step {step} in progress"
        
        st.session_state.cross_step_memory[step] = summary

    def _create_system_prompt(self, language, current_step):
        """Create step-aware system prompt for the AI based on language and current step."""
        step_context = {
            'upload': 'data upload and initial review',
            'schema': 'data schema configuration',
            'cleaning': 'data cleaning and validation',
            'weighting': 'survey weight application',
            'analysis': 'data analysis and visualization'
        }
        
        current_context = step_context.get(current_step, 'data processing')
        
        prompts = {
            'english': f"""You are a Step-Aware Process Intelligence Assistant for a Survey Data Processing Platform. 

CURRENT STEP: {current_step.upper()} - {current_context}

Your role is to:
1. PRIORITIZE the current step ({current_step}) in your responses
2. Remember and reference previous completed steps when relevant
3. Provide step-specific guidance and insights
4. Answer questions about methodology and results across all steps
5. Give actionable, step-appropriate suggestions

Always acknowledge the current processing step and provide contextual help. Be conversational, specific with numbers, and reference cross-step information when asked.""",

            'hindi': f"""à¤†à¤ª à¤à¤• Step-Aware Survey Data Processing Platform à¤•à¥‡ à¤²à¤¿à¤ Process Intelligence Assistant à¤¹à¥ˆà¤‚à¥¤

à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤šà¤°à¤£: {current_step.upper()} - {current_context}

à¤†à¤ªà¤•à¥€ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤¹à¥ˆ:
1. à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤šà¤°à¤£ ({current_step}) à¤•à¥‹ à¤ªà¥à¤°à¤¾à¤¥à¤®à¤¿à¤•à¤¤à¤¾ à¤¦à¥‡à¤‚
2. à¤ªà¤¿à¤›à¤²à¥‡ completed steps à¤•à¥‹ à¤¯à¤¾à¤¦ à¤°à¤–à¥‡à¤‚ à¤”à¤° reference à¤•à¤°à¥‡à¤‚
3. Step-specific guidance à¤”à¤° insights à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¥‡à¤‚
4. à¤¸à¤­à¥€ steps à¤•à¥‡ methodology à¤”à¤° results à¤•à¥‡ questions à¤•à¤¾ answer à¤¦à¥‡à¤‚
5. Actionable, step-appropriate suggestions à¤¦à¥‡à¤‚

à¤¹à¤®à¥‡à¤¶à¤¾ current processing step à¤•à¥‹ acknowledge à¤•à¤°à¥‡à¤‚ à¤”à¤° contextual help à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¥‡à¤‚à¥¤""",

            'gujarati': f"""àª¤àª®à«‡ Step-Aware Survey Data Processing Platform àª®àª¾àªŸà«‡ Process Intelligence Assistant àª›à«‹.

àª¹àª¾àª²àª¨à«àª‚ àªªàª—àª²à«àª‚: {current_step.upper()} - {current_context}

àª¤àª®àª¾àª°à«€ àª­à«‚àª®àª¿àª•àª¾ àª›à«‡:
1. àª¹àª¾àª²àª¨àª¾ àªªàª—àª²àª¾ ({current_step}) àª¨à«‡ àªªà«àª°àª¾àª¥àª®àª¿àª•àª¤àª¾ àª†àªªà«‹
2. àªªàª¹à«‡àª²àª¾àª¨àª¾ completed steps àª¨à«‡ àª¯àª¾àª¦ àª°àª¾àª–à«‹ àª…àª¨à«‡ reference àª•àª°à«‹
3. Step-specific guidance àª…àª¨à«‡ insights àª†àªªà«‹
4. àª¬àª§àª¾ steps àª¨àª¾ methodology àª…àª¨à«‡ results àªµàª¿àª¶à«‡ questions àª¨à«‹ answer àª†àªªà«‹
5. Actionable, step-appropriate suggestions àª†àªªà«‹

àª¹àª‚àª®à«‡àª¶àª¾ current processing step àª¨à«‡ acknowledge àª•àª°à«‹ àª…àª¨à«‡ contextual help àª†àªªà«‹."""
        }
        
        return prompts.get(language, prompts['english'])
    
    def _get_fallback_response(self, user_message, language, current_step):
        """Provide step-aware fallback responses when AI is unavailable."""
        context = self._get_step_context(current_step)
        
        fallback_responses = {
            'english': {
                'upload': f"I'm here to help with data upload. Current status: {context.get('step_status', 'pending')}. What would you like to know about uploading your survey data?",
                'schema': f"I can help with schema configuration. Your data has {context['data_info'].get('total_columns', 0)} columns. Need help mapping your data structure?",
                'cleaning': f"I'm assisting with data cleaning. Your dataset has {context['data_info'].get('missing_percentage', 0):.1f}% missing values. What cleaning questions do you have?",
                'weighting': f"I can help with survey weighting. Your cleaned data has {context.get('cleaning_results', {}).get('rows_after_cleaning', 0)} rows. Ready for weight application?",
                'analysis': f"I'm here for analysis support. All processing steps are complete. What would you like to analyze or understand about your results?"
            },
            'hindi': {
                'upload': f"à¤®à¥ˆà¤‚ data upload à¤®à¥‡à¤‚ help à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¥‚à¤‚à¥¤ Current status: {context.get('step_status', 'pending')}à¥¤ Survey data upload à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤•à¥à¤¯à¤¾ à¤œà¤¾à¤¨à¤¨à¤¾ à¤¹à¥ˆ?",
                'schema': f"à¤®à¥ˆà¤‚ schema configuration à¤®à¥‡à¤‚ help à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥‚à¤‚à¥¤ à¤†à¤ªà¤•à¥‡ data à¤®à¥‡à¤‚ {context['data_info'].get('total_columns', 0)} columns à¤¹à¥ˆà¤‚à¥¤ Data structure mapping à¤®à¥‡à¤‚ help à¤šà¤¾à¤¹à¤¿à¤?",
                'cleaning': f"à¤®à¥ˆà¤‚ data cleaning à¤®à¥‡à¤‚ help à¤•à¤° à¤°à¤¹à¤¾ à¤¹à¥‚à¤‚à¥¤ à¤†à¤ªà¤•à¥‡ dataset à¤®à¥‡à¤‚ {context['data_info'].get('missing_percentage', 0):.1f}% missing values à¤¹à¥ˆà¤‚à¥¤ Cleaning à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤•à¥à¤¯à¤¾ questions à¤¹à¥ˆà¤‚?",
                'weighting': f"à¤®à¥ˆà¤‚ survey weighting à¤®à¥‡à¤‚ help à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥‚à¤‚à¥¤ à¤†à¤ªà¤•à¥‡ cleaned data à¤®à¥‡à¤‚ {context.get('cleaning_results', {}).get('rows_after_cleaning', 0)} rows à¤¹à¥ˆà¤‚à¥¤ Weight application à¤•à¥‡ à¤²à¤¿à¤ ready à¤¹à¥ˆà¤‚?",
                'analysis': f"à¤®à¥ˆà¤‚ analysis support à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¥‚à¤‚à¥¤ à¤¸à¤­à¥€ processing steps complete à¤¹à¥ˆà¤‚à¥¤ Results à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤•à¥à¤¯à¤¾ analyze à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚?"
            },
            'gujarati': {
                'upload': f"àª¹à«àª‚ data upload àª®àª¾àª‚ help àª®àª¾àªŸà«‡ àª›à«àª‚à¥¤ Current status: {context.get('step_status', 'pending')}. Survey data upload àªµàª¿àª¶à«‡ àª¶à«àª‚ àªœàª¾àª£àªµà«àª‚ àª›à«‡?",
                'schema': f"àª¹à«àª‚ schema configuration àª®àª¾àª‚ help àª•àª°à«€ àª¶àª•à«àª‚ àª›à«àª‚à¥¤ àª¤àª®àª¾àª°àª¾ data àª®àª¾àª‚ {context['data_info'].get('total_columns', 0)} columns àª›à«‡à¥¤ Data structure mapping àª®àª¾àª‚ help àªœà«‹àªˆàª?",
                'cleaning': f"àª¹à«àª‚ data cleaning àª®àª¾àª‚ help àª•àª°à«€ àª°àª¹à«àª¯à«‹ àª›à«àª‚à¥¤ àª¤àª®àª¾àª°àª¾ dataset àª®àª¾àª‚ {context['data_info'].get('missing_percentage', 0):.1f}% missing values àª›à«‡à¥¤ Cleaning àªµàª¿àª¶à«‡ àª¶à«àª‚ questions àª›à«‡?",
                'weighting': f"àª¹à«àª‚ survey weighting àª®àª¾àª‚ help àª•àª°à«€ àª¶àª•à«àª‚ àª›à«àª‚à¥¤ àª¤àª®àª¾àª°àª¾ cleaned data àª®àª¾àª‚ {context.get('cleaning_results', {}).get('rows_after_cleaning', 0)} rows àª›à«‡à¥¤ Weight application àª®àª¾àªŸà«‡ ready àª›à«‹?",
                'analysis': f"àª¹à«àª‚ analysis support àª®àª¾àªŸà«‡ àª›à«àª‚à¥¤ àª¬àª§àª¾ processing steps complete àª›à«‡à¥¤ Results àªµàª¿àª¶à«‡ àª¶à«àª‚ analyze àª•àª°àªµà«àª‚ àª›à«‡?"
            }
        }
        
        responses = fallback_responses.get(language, fallback_responses['english'])
        return responses.get(current_step, responses['upload'])
    
    def _get_ai_response(self, user_message, language, current_step):
        """Get response from Groq AI with comprehensive error handling."""
        if not self.client:
            return self._get_fallback_response(user_message, language, current_step)
        
        try:
            context = self._get_step_context(current_step)
            system_prompt = self._create_system_prompt(language, current_step)
            
            # Create compact context string to avoid token limits
            context_str = f"""
            Step: {current_step}
            Status: {context.get('step_status', 'pending')}
            Data: {context.get('data_info', {})}
            Previous Steps: {list(st.session_state.cross_step_memory.keys())}
            """
            
            # Prepare messages for Groq
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Context: {context_str}\n\nUser Question: {user_message}"}
            ]
            
            # Call Groq API with optimized settings
            response = self.client.chat.completions.create(
                model="llama-3.1-8b-instant",  # Fastest model for better performance
                messages=messages,
                max_tokens=400,  # Reduced for faster responses
                temperature=0.6,
                timeout=8  # 8 second timeout for faster response
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            # Log error and provide fallback
            return self._get_fallback_response(user_message, language, current_step)

    def display_chatbot(self):
        """Display the Step-Aware Process Intelligence Chatbot interface."""
        st.markdown("---")
        
        # Update current step
        self.current_step = self._detect_current_step()
        
        # Ensure current step has message history
        if self.current_step not in st.session_state.step_chat_messages:
            st.session_state.step_chat_messages[self.current_step] = []
        
        # Header with step awareness
        step_names = {
            'upload': 'ğŸ“ Data Upload',
            'schema': 'ğŸ—‚ï¸ Schema Setup', 
            'cleaning': 'ğŸ§¹ Data Cleaning',
            'weighting': 'âš–ï¸ Weight Application',
            'analysis': 'ğŸ“ˆ Analysis & Visualization'
        }
        
        current_step_name = step_names.get(self.current_step, self.current_step.title())
        st.subheader(f"ğŸ¤– Process Assistant - {current_step_name}")
        
        # Step-aware welcome message for first query
        if self.current_step not in st.session_state.step_first_query:
            welcome_messages = {
                'english': {
                    'upload': "ğŸ‘‹ Welcome! I'm here to help you upload and review your survey data. What would you like to know?",
                    'schema': "ğŸ—‚ï¸ Great! Now I can help you configure your data schema. Any questions about your data structure?", 
                    'cleaning': "ğŸ§¹ Perfect! I'm ready to assist with data cleaning. What cleaning questions do you have?",
                    'weighting': "âš–ï¸ Excellent! I can help you apply survey weights. Ready to discuss weighting strategies?",
                    'analysis': "ğŸ“ˆ Fantastic! All steps are complete. I can help you understand and analyze your results."
                }
            }
            
            language = st.session_state.chatbot_language
            welcome_msg = welcome_messages.get(language, welcome_messages['english']).get(self.current_step, "Hello! How can I help you?")
            
            # Add welcome message to chat
            st.session_state.step_chat_messages[self.current_step].append({
                "role": "assistant", 
                "content": welcome_msg,
                "timestamp": datetime.now().strftime("%H:%M")
            })
            st.session_state.step_first_query[self.current_step] = True

        # Display chat messages using streamlit chat UI
        for message in st.session_state.step_chat_messages[self.current_step]:
            with st.chat_message(message["role"]):
                st.write(message["content"])
                st.caption(f"â° {message['timestamp']}")

        # Chat input
        if prompt := st.chat_input(f"Ask about {current_step_name.lower()}..."):
            # Add user message to chat
            user_msg = {
                "role": "user", 
                "content": prompt,
                "timestamp": datetime.now().strftime("%H:%M")
            }
            st.session_state.step_chat_messages[self.current_step].append(user_msg)
            
            # Display user message
            with st.chat_message("user"):
                st.write(prompt)
                st.caption(f"â° {user_msg['timestamp']}")

            # Generate AI response
            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤” Thinking..."):
                    language = st.session_state.chatbot_language
                    ai_response = self._get_ai_response(prompt, language, self.current_step)
                
                st.write(ai_response)
                timestamp = datetime.now().strftime("%H:%M")
                st.caption(f"â° {timestamp}")
                
                # Add AI response to chat
                assistant_msg = {
                    "role": "assistant", 
                    "content": ai_response,
                    "timestamp": timestamp
                }
                st.session_state.step_chat_messages[self.current_step].append(assistant_msg)

        # Language selector in sidebar
        with st.sidebar:
            st.markdown("**ğŸŒ Chat Language**")
            language_options = {
                'English': 'english',
                'à¤¹à¤¿à¤¨à¥à¤¦à¥€': 'hindi', 
                'àª—à«àªœàª°àª¾àª¤à«€': 'gujarati'
            }
            
            selected_lang = st.selectbox(
                "Language",
                options=list(language_options.keys()),
                index=list(language_options.values()).index(st.session_state.chatbot_language),
                key="chatbot_language_selector"
            )
            st.session_state.chatbot_language = language_options[selected_lang]

        # Cross-step navigation in sidebar
        with st.sidebar:
            st.markdown("**ğŸ“‹ Step History**")
            for step, summary in st.session_state.cross_step_memory.items():
                status_icon = "âœ…" if summary['completed'] else "â³"
                st.write(f"{status_icon} **{step.title()}**: {summary['summary']}")

        # Clear chat option
        if st.button("ğŸ—‘ï¸ Clear Current Step Chat", key="clear_step_chat"):
            st.session_state.step_chat_messages[self.current_step] = []
            if self.current_step in st.session_state.step_first_query:
                del st.session_state.step_first_query[self.current_step]
            st.rerun()