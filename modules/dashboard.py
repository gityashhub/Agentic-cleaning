import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta

class Dashboard:
    """Comprehensive analytics dashboard for the Survey Data Processing Platform."""
    
    def __init__(self):
        pass
    
    def display_dashboard(self):
        """Main dashboard interface."""
        st.header("ğŸ“Š Analytics Dashboard")
        st.markdown("*Real-time insights into your data processing workflow*")
        
        # Check if we have data to analyze
        if st.session_state.get('data') is None:
            self._display_empty_dashboard()
            return
        
        # Main dashboard with tabs
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ Data Overview", "ğŸ” Quality Metrics", "âš–ï¸ Processing Pipeline", "ğŸ¯ Performance Insights"])
        
        with tab1:
            self._display_data_overview()
        
        with tab2:
            self._display_quality_metrics()
        
        with tab3:
            self._display_processing_pipeline()
        
        with tab4:
            self._display_performance_insights()
    
    def _display_empty_dashboard(self):
        """Display dashboard when no data is available."""
        st.info("ğŸš€ Upload data to unlock the full analytics dashboard!")
        
        # Show demo metrics
        st.subheader("ğŸ“Š Demo Dashboard Preview")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Data Files Processed", "0", help="Number of datasets uploaded and processed")
        with col2:
            st.metric("Total Records", "0", help="Total number of survey records processed")
        with col3:
            st.metric("Cleaning Actions", "0", help="Number of data cleaning operations performed")
        with col4:
            st.metric("Reports Generated", "0", help="Number of analysis reports created")
        
        # Sample visualization
        st.subheader("ğŸ“ˆ Sample Analytics")
        sample_data = pd.DataFrame({
            'Processing Stage': ['Upload', 'Cleaning', 'Weighting', 'Analysis', 'Reporting'],
            'Completion Rate': [100, 0, 0, 0, 0]
        })
        
        fig = px.funnel(sample_data, x='Completion Rate', y='Processing Stage', 
                       title="Processing Pipeline Progress")
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
    
    def _display_data_overview(self):
        """Display data overview metrics and visualizations."""
        data = st.session_state.data
        
        # Key metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Records", f"{data.shape[0]:,}")
        with col2:
            st.metric("Total Columns", data.shape[1])
        with col3:
            numeric_cols = len(data.select_dtypes(include=[np.number]).columns)
            st.metric("Numeric Columns", numeric_cols)
        with col4:
            missing_pct = (data.isnull().sum().sum() / (data.shape[0] * data.shape[1]) * 100)
            st.metric("Missing Data", f"{missing_pct:.1f}%", delta=f"-{missing_pct:.1f}%" if missing_pct < 10 else None)
        
        # Data type distribution
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“Š Data Type Distribution")
            dtype_counts = data.dtypes.value_counts()
            fig = px.pie(values=dtype_counts.values, names=dtype_counts.index, 
                        title="Column Data Types")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("ğŸ“ Column Statistics")
            stats_data = []
            for col in data.columns[:10]:  # Show first 10 columns
                if data[col].dtype in ['int64', 'float64']:
                    stats_data.append({
                        'Column': col,
                        'Type': 'Numeric',
                        'Unique Values': data[col].nunique(),
                        'Missing %': f"{(data[col].isnull().sum() / len(data) * 100):.1f}%"
                    })
                else:
                    stats_data.append({
                        'Column': col,
                        'Type': 'Categorical',
                        'Unique Values': data[col].nunique(),
                        'Missing %': f"{(data[col].isnull().sum() / len(data) * 100):.1f}%"
                    })
            
            stats_df = pd.DataFrame(stats_data)
            st.dataframe(stats_df, use_container_width=True)
        
        # Missing data heatmap
        if data.isnull().sum().sum() > 0:
            st.subheader("ğŸ¯ Missing Data Pattern")
            missing_data = data.isnull()
            
            # Create heatmap of missing data
            fig = go.Figure(data=go.Heatmap(
                z=missing_data.values,
                x=missing_data.columns,
                y=list(range(len(missing_data))),
                colorscale='Reds',
                showscale=True
            ))
            fig.update_layout(title="Missing Data Heatmap (Red = Missing)", height=400)
            st.plotly_chart(fig, use_container_width=True)
    
    def _display_quality_metrics(self):
        """Display data quality metrics and alerts."""
        data = st.session_state.data
        
        st.subheader("ğŸ¯ Data Quality Assessment")
        
        # Quality score calculation
        quality_scores = self._calculate_quality_scores(data)
        
        # Overall quality score
        overall_score = np.mean(list(quality_scores.values()))
        col1, col2, col3 = st.columns([1, 2, 1])
        
        with col2:
            # Create a gauge chart for overall quality
            fig = go.Figure(go.Indicator(
                mode = "gauge+number+delta",
                value = overall_score,
                domain = {'x': [0, 1], 'y': [0, 1]},
                title = {'text': "Overall Data Quality Score"},
                delta = {'reference': 80},
                gauge = {
                    'axis': {'range': [None, 100]},
                    'bar': {'color': "darkblue"},
                    'steps': [
                        {'range': [0, 50], 'color': "lightgray"},
                        {'range': [50, 80], 'color': "gray"},
                        {'range': [80, 100], 'color': "lightgreen"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 90
                    }
                }
            ))
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
        
        # Individual quality metrics
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“Š Quality Metrics")
            for metric, score in quality_scores.items():
                if score >= 80:
                    st.success(f"âœ… {metric}: {score:.1f}%")
                elif score >= 60:
                    st.warning(f"âš ï¸ {metric}: {score:.1f}%")
                else:
                    st.error(f"âŒ {metric}: {score:.1f}%")
        
        with col2:
            st.subheader("ğŸ” Quality Issues")
            issues = self._identify_quality_issues(data)
            
            if not issues:
                st.success("ğŸ‰ No major quality issues detected!")
            else:
                for issue in issues:
                    st.warning(f"âš ï¸ {issue}")
        
        # Quality trend (if we have cleaning history)
        if st.session_state.get('processing_log'):
            st.subheader("ğŸ“ˆ Quality Improvement Trend")
            self._display_quality_trend()
    
    def _display_processing_pipeline(self):
        """Display processing pipeline status and metrics."""
        st.subheader("ğŸ”„ Processing Pipeline Status")
        
        # Pipeline stages
        stages = [
            ("ğŸ“ Data Upload", st.session_state.get('data') is not None),
            ("ğŸ—‚ï¸ Schema Mapping", st.session_state.get('schema') is not None),
            ("ğŸ§¹ Data Cleaning", st.session_state.get('cleaned_data') is not None),
            ("âš–ï¸ Weight Application", st.session_state.get('weighted_results') is not None),
            ("ğŸ“„ Report Generation", False)  # This would be checked based on report history
        ]
        
        # Progress visualization
        completed_stages = sum(1 for _, completed in stages if completed)
        progress = completed_stages / len(stages) * 100
        
        st.progress(progress / 100)
        st.write(f"**Pipeline Progress: {progress:.0f}% ({completed_stages}/{len(stages)} stages completed)**")
        
        # Stage details
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“‹ Stage Status")
            for stage_name, completed in stages:
                if completed:
                    st.success(f"âœ… {stage_name}")
                else:
                    st.info(f"â³ {stage_name}")
        
        with col2:
            st.subheader("â±ï¸ Processing Timeline")
            if st.session_state.get('processing_log'):
                timeline_data = []
                for entry in st.session_state.processing_log:
                    timeline_data.append({
                        'Timestamp': entry['timestamp'],
                        'Stage': entry['step'],
                        'Duration': '~2 min'  # This could be calculated from actual timing
                    })
                
                timeline_df = pd.DataFrame(timeline_data)
                st.dataframe(timeline_df, use_container_width=True)
            else:
                st.info("No processing history available yet.")
        
        # Resource usage (simulated)
        st.subheader("ğŸ’» Resource Usage")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Memory Usage", "42%", delta="5%")
        with col2:
            st.metric("Processing Time", "3.2s", delta="-0.8s")
        with col3:
            st.metric("API Calls", "12", delta="3")
    
    def _display_performance_insights(self):
        """Display performance insights and optimization suggestions."""
        st.subheader("ğŸ¯ Performance Insights & Recommendations")
        
        data = st.session_state.data
        
        if data is not None:
            # Performance metrics
            col1, col2, col3 = st.columns(3)
            
            with col1:
                processing_efficiency = min(100, (data.shape[0] / 10000) * 100)
                st.metric("Processing Efficiency", f"{processing_efficiency:.1f}%")
            
            with col2:
                data_completeness = (1 - data.isnull().sum().sum() / (data.shape[0] * data.shape[1])) * 100
                st.metric("Data Completeness", f"{data_completeness:.1f}%")
            
            with col3:
                validation_score = 85  # This would be calculated based on validation results
                st.metric("Validation Score", f"{validation_score}%")
            
            # Recommendations
            st.subheader("ğŸ’¡ Optimization Recommendations")
            recommendations = self._generate_recommendations(data)
            
            for i, rec in enumerate(recommendations, 1):
                st.info(f"**{i}.** {rec}")
            
            # Performance comparison
            st.subheader("ğŸ“Š Performance Comparison")
            perf_data = pd.DataFrame({
                'Metric': ['Processing Speed', 'Memory Efficiency', 'Accuracy', 'Completeness'],
                'Current': [85, 78, 92, int(data_completeness)],
                'Industry Average': [70, 65, 85, 80],
                'Best Practice': [95, 90, 98, 95]
            })
            
            fig = go.Figure()
            
            fig.add_trace(go.Scatter(
                x=perf_data['Metric'],
                y=perf_data['Current'],
                mode='lines+markers',
                name='Current Performance',
                line=dict(color='blue', width=3)
            ))
            
            fig.add_trace(go.Scatter(
                x=perf_data['Metric'],
                y=perf_data['Industry Average'],
                mode='lines+markers',
                name='Industry Average',
                line=dict(color='orange', width=2, dash='dash')
            ))
            
            fig.add_trace(go.Scatter(
                x=perf_data['Metric'],
                y=perf_data['Best Practice'],
                mode='lines+markers',
                name='Best Practice',
                line=dict(color='green', width=2, dash='dot')
            ))
            
            fig.update_layout(
                title="Performance Benchmarking",
                yaxis_title="Score (%)",
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
    
    def _calculate_quality_scores(self, data):
        """Calculate various data quality scores."""
        scores = {}
        
        # Completeness score
        total_cells = data.shape[0] * data.shape[1]
        missing_cells = data.isnull().sum().sum()
        scores['Completeness'] = ((total_cells - missing_cells) / total_cells) * 100
        
        # Consistency score (simplified)
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            # Check for outliers using IQR method
            outlier_count = 0
            for col in numeric_cols:
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = data[(data[col] < (Q1 - 1.5 * IQR)) | (data[col] > (Q3 + 1.5 * IQR))]
                outlier_count += len(outliers)
            
            scores['Consistency'] = max(0, (1 - outlier_count / len(data)) * 100)
        else:
            scores['Consistency'] = 100
        
        # Validity score (simplified - assumes most data is valid)
        scores['Validity'] = 90  # This would be based on actual validation rules
        
        # Uniqueness score
        duplicate_rows = data.duplicated().sum()
        scores['Uniqueness'] = ((len(data) - duplicate_rows) / len(data)) * 100
        
        return scores
    
    def _identify_quality_issues(self, data):
        """Identify specific data quality issues."""
        issues = []
        
        # Check for high missing data columns
        missing_pct = data.isnull().sum() / len(data) * 100
        high_missing_cols = missing_pct[missing_pct > 20].index.tolist()
        
        if high_missing_cols:
            issues.append(f"High missing data in columns: {', '.join(high_missing_cols[:3])}")
        
        # Check for duplicates
        if data.duplicated().sum() > 0:
            issues.append(f"Found {data.duplicated().sum()} duplicate rows")
        
        # Check for potential data type issues
        for col in data.columns:
            if data[col].dtype == 'object':
                if data[col].str.isnumeric().sum() > len(data) * 0.8:
                    issues.append(f"Column '{col}' might need numeric conversion")
        
        return issues
    
    def _display_quality_trend(self):
        """Display quality improvement trend over processing steps."""
        # This would show how quality metrics improve through the pipeline
        steps = ['Raw Data', 'After Cleaning', 'After Validation', 'Final']
        quality_scores = [65, 78, 85, 92]  # These would be calculated from actual processing
        
        fig = px.line(x=steps, y=quality_scores, title="Data Quality Improvement",
                     markers=True, line_shape='spline')
        fig.update_layout(yaxis_title="Quality Score (%)", height=300)
        st.plotly_chart(fig, use_container_width=True)
    
    def _generate_recommendations(self, data):
        """Generate optimization recommendations based on data analysis."""
        recommendations = []
        
        # Missing data recommendations
        missing_pct = data.isnull().sum().sum() / (data.shape[0] * data.shape[1]) * 100
        if missing_pct > 10:
            recommendations.append("Consider using KNN imputation for better missing value handling")
        
        # Large dataset recommendations
        if data.shape[0] > 50000:
            recommendations.append("For large datasets, consider sampling for faster processing")
        
        # Memory optimization
        if data.shape[1] > 50:
            recommendations.append("Consider feature selection to reduce dimensionality")
        
        # General recommendations
        recommendations.append("Implement data validation rules for automated quality checking")
        recommendations.append("Set up automated backup of processed data")
        
        return recommendations